# Story 1.1: Web Data Extraction Foundation

## Story Info

- **Epic:** 1 - Core Automation Engine  
- **Story Number:** 1.1  
- **Status:** Ready for Development  
- **Created:** 2025-07-29  

## Story Statement

**As a** developer, **I want** to create a Python module that can navigate to a URL and extract a specified data table from an HTML page, **so that** MAGK can handle web-based data sources.

## Acceptance Criteria

1. The module accepts a URL and a table identifier.
2. The module successfully returns the data from the specified table.
3. The module can handle basic HTML table structures.

## Dev Notes

### Previous Story Insights
No previous story exists - this is the first story in the project.

### Data Models
**WorkflowConfig**: This story will work with the WorkflowConfig data model which includes:
- `sourceType`: Will be 'web' for this module
- `sourceUri`: URL to the website containing the data table
- `dataIdentifier`: String to identify the specific table (e.g., table ID or text to find)
[Source: architecture/data-models.md#WorkflowConfig]

### API Specifications
This module will be called from the `/execute-workflow` endpoint:
- **Purpose**: To trigger the data extraction workflow
- **Request Body**: The WorkflowConfig object in JSON format
- **Response**: `{ "outputUrl": "<https://s3>..." }` (A pre-signed URL to download the generated Excel file)
[Source: architecture/api-specification.md#POST /execute-workflow]

### Component Specifications
**Web Scraping Technology**: Selenium 4.x will be used for browser automation and web data extraction
- **Rationale**: Handles dynamic websites effectively
- **Location**: This module will be part of the serverless backend in the chalicelib directory
[Source: architecture/tech-stack.md#Web Scraping]

### File Locations
Based on the project structure, the web data extraction module should be created at:
- **Backend Module**: `apps/server/chalicelib/web_extractor.py`
- **Main API Logic**: Integration in `apps/server/app.py`
- **Tests**: `apps/server/tests/test_web_extractor.py`
[Source: architecture/unified-project-structure.md#Project Structure]

### Testing Requirements
**Unit Testing**: 
- Automated unit tests using `pytest` will be implemented for the core web extraction module
- Tests will be located in the server tests directory
- Primary focus on foundational reliability
[Source: architecture/testing-strategy.md#Unit Testing]

### Technical Constraints
**Python Version**: Python 3.10.x is required for consistency with the backend
**AWS Platform**: Module will run on AWS Lambda functions
**Error Handling**: Use standard Python try/except blocks with CloudWatch logging
[Source: architecture/tech-stack.md#Backend Language, architecture/error-handling-strategy.md#Server-Side]

### Security and Performance Considerations
**HTTPS Communication**: All communication must be over HTTPS
**Performance**: Heavy processing offloaded to backend to keep UI responsive
**AWS Lambda Scaling**: Automatic scaling with Provisioned Concurrency for demo
[Source: architecture/security-and-performance.md#Security, architecture/security-and-performance.md#Performance]

## Tasks / Subtasks

### Task 1: Set up web extraction module structure (AC: 1)
1.1. Create `apps/server/chalicelib/web_extractor.py` module file
1.2. Define module interface accepting URL and table identifier parameters
1.3. Set up basic error handling using try/except blocks
1.4. Add logging integration for CloudWatch debugging

### Task 2: Implement Selenium-based web navigation (AC: 1, 2)
2.1. Configure Selenium WebDriver for headless operation (AWS Lambda compatible)
2.2. Implement URL navigation functionality
2.3. Add basic error handling for network connectivity issues
2.4. Implement timeout mechanisms for page loading

### Task 3: Implement HTML table extraction logic (AC: 2, 3)
3.1. Create table identification logic using the provided dataIdentifier
3.2. Implement HTML table parsing to extract tabular data
3.3. Handle basic HTML table structures (thead, tbody, tr, td elements)
3.4. Return data in a structured format (list of lists or similar)

### Task 4: Integrate with backend API workflow (AC: 1, 2)
4.1. Integrate web extractor into the Chalice app.py execute-workflow endpoint
4.2. Handle WorkflowConfig object parsing for web source type  
4.3. Ensure proper response formatting for the API specification
4.4. Add error handling for malformed requests

### Task 5: Unit testing implementation
5.1. Create `apps/server/tests/test_web_extractor.py` test file
5.2. Write unit tests for URL navigation functionality
5.3. Write unit tests for table extraction logic
5.4. Write unit tests for error handling scenarios
5.5. Create mock test data and fixtures for testing
5.6. Ensure all tests pass using pytest

### Task 6: AWS Lambda compatibility verification
6.1. Verify Selenium configuration works in serverless environment
6.2. Test module performance within Lambda execution limits
6.3. Validate error logging to CloudWatch
6.4. Ensure module meets AWS Lambda deployment requirements

## Project Structure Notes

The current project structure aligns with the defined architecture. The web extraction module will be placed in the `apps/server/chalicelib/` directory as specified in the unified project structure guide. No structural conflicts identified.

## Dev Agent Record

### Implementation Notes
*To be filled in by the Dev Agent during implementation*

### Completion Notes  
*To be filled in by the Dev Agent upon story completion*

### Debug Log References
*References to debug log entries will be added here during implementation*

---
*Story created: 2025-07-29*